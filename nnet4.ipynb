{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayathu/anaconda3/envs/ml_35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from my_classes import DataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "model_from_json = keras.models.model_from_json\n",
    "\n",
    "types = ['kmno4', 'set1', 'set2', 'set5', 'set9']\n",
    "ModelCheckpoint=keras.callbacks.ModelCheckpoint\n",
    "Sequential=keras.models.Sequential\n",
    "Dense=keras.layers.Dense\n",
    "Flatten=keras.layers.Flatten\n",
    "Dropout=keras.layers.Dropout\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of creating a data generator is to avoid unnecessary usage of RAM when using larger data sets. `my_classes.py` contains such a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating `partition` dictionary"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{'train':[['filename', 'position'], ['filename2','position2']]}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "l=[]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for matfile in glob.glob('unsieved_concat/*.mat'):\n",
    "    for i in range(56250-32):\n",
    "        l.append([matfile,i])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "np.random.shuffle(l)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "l[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "total_data = len(l)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "train_size = int(total_data * 70 / 100)\n",
    "train_size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "val_size = int((total_data - train_size) / 2)\n",
    "val_size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "test_size = val_size\n",
    "test_size"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "partition={'train':l[:train_size],\n",
    "           'val':l[train_size:train_size+val_size],\n",
    "           'test':l[train_size+val_size:train_size+val_size+test_size]}"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "with open('nnet2_partition.pickle', 'wb') as handle:\n",
    "    pickle.dump(partition, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading `partition` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nnet2_partition.pickle', 'rb') as handle:\n",
    "    partition=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved dictionary is 90MB as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r--. 1 jayathu jayathu 90M Sep  8 09:18 nnet2_partition.pickle\n"
     ]
    }
   ],
   "source": [
    "!ls -lh nnet2_partition.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dim': (16,32),\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 5,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators\n",
    "training_generator = DataGenerator(partition['train'], [], **params)\n",
    "validation_generator = DataGenerator(partition['val'], [], **params)\n",
    "testing_generator = DataGenerator(partition['test'], [], **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(*params['dim'],1)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"models/nnet4/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.90808, saving model to models/nnet4/weights-improvement-01-0.91.hdf5\n"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=4,\n",
    "                    epochs=5,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"models/nnet4/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('models/nnet4/model.json', 'r')\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights('models/nnet4/weights-improvement-03-0.92.hdf5')\n",
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if model is alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21608/21608 [==============================] - 1296s 60ms/step\n",
      "\n",
      "acc: 92.08%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores=model.evaluate_generator(testing_generator, steps=None, max_queue_size=10, workers=4, use_multiprocessing=1, verbose=1)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
