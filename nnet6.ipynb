{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jayathu/anaconda3/envs/ml_35/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from scipy.io import loadmat\n",
    "from my_classes import DataGenerator2\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import pickle\n",
    "%matplotlib inline\n",
    "\n",
    "model_from_json = keras.models.model_from_json\n",
    "\n",
    "types = ['kmno4', 'set1', 'set2', 'set5', 'set9']\n",
    "ModelCheckpoint=keras.callbacks.ModelCheckpoint\n",
    "Sequential=keras.models.Sequential\n",
    "Dense=keras.layers.Dense\n",
    "Flatten=keras.layers.Flatten\n",
    "Dropout=keras.layers.Dropout\n",
    "np.random.seed(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of creating a data generator is to avoid unnecessary usage of RAM when using larger data sets. `my_classes.py` contains such a generator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating `partition` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'train':[['filename', 'position'], ['filename2','position2']]}\n",
    "\n",
    "# l=[]\n",
    "\n",
    "# for matfile in glob.glob('unsieved_concat/*.mat'):\n",
    "#     for i in range(56250-64):\n",
    "#         l.append([matfile,i])\n",
    "\n",
    "# np.random.shuffle(l)\n",
    "\n",
    "# l[:5]\n",
    "\n",
    "# total_data = len(l)\n",
    "\n",
    "# train_size = int(total_data * 70 / 100)\n",
    "# train_size\n",
    "\n",
    "# val_size = int((total_data - train_size) / 2)\n",
    "# val_size\n",
    "\n",
    "# test_size = val_size\n",
    "# test_size\n",
    "\n",
    "# partition={'train':l[:train_size],\n",
    "#            'val':l[train_size:train_size+val_size],\n",
    "#            'test':l[train_size+val_size:train_size+val_size+test_size]}\n",
    "\n",
    "# with open('nnet6_partition.pickle', 'wb') as handle:\n",
    "#     pickle.dump(partition, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading `partition` dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nnet6_partition.pickle', 'rb') as handle:\n",
    "    partition=pickle.load(handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The saved dictionary is 90MB as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-rw-r--. 1 jayathu jayathu 91M Sep 10 12:30 nnet6_partition.pickle\n"
     ]
    }
   ],
   "source": [
    "!ls -lh nnet6_partition.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'dim': (16,64),\n",
    "          'batch_size': 64,\n",
    "          'n_classes': 5,\n",
    "          'n_channels': 1,\n",
    "          'shuffle': True}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generators\n",
    "training_generator = DataGenerator2(partition['train'], [], **params)\n",
    "validation_generator = DataGenerator2(partition['val'], [], **params)\n",
    "testing_generator = DataGenerator2(partition['test'], [], **params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Flatten(input_shape=(*params['dim'],1)))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dropout(.25))\n",
    "model.add(Dense(5, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath=\"models/nnet6/weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "100781/100783 [============================>.] - ETA: 0s - loss: 0.2708 - acc: 0.8791\n",
      "Epoch 00001: val_acc improved from -inf to 0.91084, saving model to models/nnet6/weights-improvement-01-0.91.hdf5\n",
      "100783/100783 [==============================] - 7412s 74ms/step - loss: 0.2708 - acc: 0.8791 - val_loss: 0.1918 - val_acc: 0.9108\n",
      "Epoch 2/5\n",
      "100782/100783 [============================>.] - ETA: 0s - loss: 0.1944 - acc: 0.9142\n",
      "Epoch 00002: val_acc improved from 0.91084 to 0.92354, saving model to models/nnet6/weights-improvement-02-0.92.hdf5\n",
      "100783/100783 [==============================] - 7398s 73ms/step - loss: 0.1944 - acc: 0.9142 - val_loss: 0.1712 - val_acc: 0.9235\n",
      "Epoch 3/5\n",
      " 23379/100783 [=====>........................] - ETA: 1:18:24 - loss: 0.1822 - acc: 0.9201"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 53731/100783 [==============>...............] - ETA: 47:40 - loss: 0.1812 - acc: 0.9207\n",
      "Epoch 00003: val_acc improved from 0.92354 to 0.93116, saving model to models/nnet6/weights-improvement-03-0.93.hdf5\n",
      "100783/100783 [==============================] - 7434s 74ms/step - loss: 0.1788 - acc: 0.9220 - val_loss: 0.1632 - val_acc: 0.9312\n",
      "Epoch 4/5\n",
      " 78517/100783 [======================>.......] - ETA: 22:39 - loss: 0.1706 - acc: 0.9257"
     ]
    }
   ],
   "source": [
    "model.fit_generator(generator=training_generator,\n",
    "                    validation_data=validation_generator,\n",
    "                    use_multiprocessing=True,\n",
    "                    workers=4,\n",
    "                    epochs=5,\n",
    "                    callbacks=callbacks_list,\n",
    "                    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"models/nnet6/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate model with test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('models/nnet6/model.json', 'r')\n",
    "model_json = json_file.read()\n",
    "json_file.close()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights('models/nnet6/weights-improvement-03-0.93.hdf5')\n",
    "# Compile model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check if model is alright."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21596/21596 [==============================] - 1311s 61ms/step\n",
      "\n",
      "acc: 93.10%\n"
     ]
    }
   ],
   "source": [
    "# evaluate the model\n",
    "scores=model.evaluate_generator(testing_generator, steps=None, max_queue_size=10, workers=4, use_multiprocessing=1, verbose=1)\n",
    "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
